\chapter{Discussão e Resultados: Chu-liu/Edmonds vs. Frank}

Neste capítulo, apresentamos uma análise comparativa entre os algoritmos de Chu--Liu--Edmonds e András Frank para o problema da arborescência de custo mínimo. Ambos produzem soluções ótimas, mas diferem em sua organização: Chu--Liu--Edmonds opera vértice a vértice com contrações imediatas de ciclos, enquanto Frank identifica subconjuntos minimais via componentes fortemente conexas, processando múltiplos vértices simultaneamente em duas fases distintas. O objetivo é elucidar essas diferenças e avaliar empiricamente seus desempenhos.

\section{Análise comparativa dos algoritmos}

O algoritmo de Chu--Liu--Edmonds \cite{chu1965,edmonds1967optimum} opera recursivamente selecionando para cada vértice \(v \neq r\) um arco de entrada de custo mínimo, contraindo ciclos detectados e ajustando custos, até eliminar todos os ciclos.

O algoritmo de Frank \cite{frank1981,frank2014} também reduz custos subtraindo o mínimo de entrada, mas identifica \emph{subconjuntos minimais} via componentes fortemente conexas, processando múltiplos vértices simultaneamente. Opera em duas fases: (i) redução até criar arcos de custo reduzido zero e (ii) construção da arborescência a partir desses arcos.

Ambos contraem ciclos detectados e expandem ao final, com número de contrações limitado por \(O(n)\) \cite{schrijver2003comb}.

Logo, os métodos são equivalentes e resolvem o problema de arborescência de custo mínimo, mas adotam estratégias distintas na redução de custos e na construção da solução e nessa sessão experimental buscamos avaliar essas diferenças na prática.

\section{Avaliação experimental}

Conduzimos 2000 experimentos em digrafos enraizados aleatórios com \(|V| \in [100, 200]\) vértices, densidade \(|A| \in [n, 3n]\) arcos e pesos inteiros em \([1, 20]\), garantindo conectividade por construção incremental \cite{schrijver2003comb}. Para cada instância, executamos Chu--Liu--Edmonds e as duas fases de Frank (com variantes v1 e v2 da Fase~II), registrando custos, tempos e métricas estruturais.

\subsection{Resultados}

Os 2000 testes confirmaram que todos os métodos retornam sempre o mesmo custo ótimo, com 100\% de sucesso, validando a corretude das implementações e a equivalência teórica entre Chu--Liu/Edmonds e Frank \cite{frank2014,schrijver2003comb}. Além disso, as verificações da condição de otimalidade dual foram bem-sucedidas em todos os casos para ambas as variantes de Frank.

Quanto ao desempenho temporal, observamos que Chu--Liu/Edmonds apresenta desempenho computacional superior nas instâncias testadas, com tempo mediano significativamente menor que a Fase~I de Frank. A Fase~I, responsável pela identificação de subconjuntos minimais e redução de custos, domina o tempo total de execução do método de Frank, enquanto as Fases~II representam uma fração residual do processamento.

A comparação entre as duas variantes da Fase~II revela ganho expressivo com o uso de \emph{heap} (fila de prioridade). A versão v2 apresenta aceleração (\emph{speedup}) consistente sobre a versão v1, confirmando empiricamente a vantagem da estrutura de dados com complexidade \(O(\log n)\) versus \(O(n)\) por operação de seleção de mínimo.

As métricas estruturais mostram que o número de contrações em Chu--Liu/Edmonds é pequeno (mediana de 1 e média de 2,29), muito abaixo do limite teórico \(O(n)\) \cite{schrijver2003comb}, o que pode indicar que digrafos aleatórios com distribuição uniforme raramente apresentam estruturas cíclicas complexas. O subgrafo \(D_0\) produzido pela Fase~I cresce linearmente com o número de vértices, confirmando a proporcionalidade \(|A_0| = O(|V|)\) prevista teoricamente.

O consumo de memória na Fase~I mantém-se modesto em todas as instâncias testadas, viabilizando a aplicação dos algoritmos mesmo em ambientes com recursos limitados.

As Figuras~\ref{fig:times-boxplot}--\ref{fig:d0-vs-v} apresentam os resultados experimentais.

\begin{figure}[H]
    \centering
    \includegraphics[width=.85\linewidth]{figures/fig_times_boxplot.png}
    \caption{Distribuição de tempos: Fase~I apresenta maior mediana (\(0{,}084\) s) e variabilidade.}
    \label{fig:times-boxplot}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.85\linewidth]{figures/fig_time_vs_edges_scatter.png}
    \caption{Escalonamento temporal em função de \(|A|\): crescimento aproximadamente linear.}
    \label{fig:time-vs-edges}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{figures/fig_speedup_hist.png}
    \caption{Aceleração (\emph{speedup}) na Fase~II: v2 (\emph{heap}/fila de prioridade) \(4{,}59\times\) mais rápida que v1.}
    \label{fig:speedup}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.48\linewidth]{figures/fig_contractions_depth.png}
    \caption{Contrações e profundidade em Chu--Liu: mediana 1, média \(2{,}29\).}
    \label{fig:contr-depth}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{figures/fig_peakmem_hist.png}
    \caption{Pico de memória na Fase~I: mediana \(539\) kB.}
    \label{fig:peakmem}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{figures/fig_d0_edges_vs_vertices.png}
    \caption{Tamanho de \(D_0\) versus \(|V|\): relação linear confirma \(|A_0| = O(|V|)\).}
    \label{fig:d0-vs-v}
\end{figure}

\subsection{Conclusões}

Os experimentos validam empiricamente as previsões teóricas e revelam características importantes do comportamento prático dos algoritmos. A equivalência de custos em 100\% das instâncias, somada à verificação bem-sucedida das condições de otimalidade dual, confirma a corretude das implementações.

Chu--Liu/Edmonds demonstra-se mais eficiente para construção direta de arborescências nas instâncias aleatórias testadas, enquanto o método de Frank apresenta overhead na Fase~I devido ao processamento de subconjuntos minimais via componentes fortemente conexas. A versão heap da Fase~II, por sua vez, valida os ganhos assintóticos esperados, demonstrando que melhorias algorítmicas fundamentais se traduzem em benefícios práticos mensuráveis.

O comportamento em digrafos aleatórios é significativamente melhor que os limites teóricos de pior caso, especialmente quanto ao número de contrações. O consumo modesto de memória e a escalabilidade observada viabilizam a aplicação prática de ambos os métodos em contextos com recursos computacionais limitados.

Compreender os algoritmos teoricamente e validá-los empiricamente é fundamental, mas como transformar esse conhecimento em aprendizagem efetiva? Desenvolvemos uma aplicação \textit{web} que permite acompanhar passo a passo o funcionamento de ambos os algoritmos de forma visual e interativa. O próximo capítulo discute os fundamentos didáticos que orientaram esse design.
